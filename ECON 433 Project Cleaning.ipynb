{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b6a4eaf46dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning part 1: create new song ID with instances and flag first and last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "songs = pd.read_csv('Hot Stuff.csv')\n",
    "\n",
    "#turn weekID to date\n",
    "songs.WeekID = pd.to_datetime(songs.WeekID)\n",
    "\n",
    "#print(songs.info())\n",
    "songs_trunc = songs.sort_values(by = ['SongID', 'Instance', 'Weeks on Chart'])\n",
    "\n",
    "#creating songid_2\n",
    "new = songs_trunc['Instance'].astype(str).copy()\n",
    "#print (songs_trunc.info())\n",
    "songs_trunc['SongID_2'] = songs_trunc['SongID'].str.cat(new, sep = \" \")\n",
    "#print(songs_trunc['SongID_2'])\n",
    "\n",
    "#creating flag for first and last occurence (takes into account instances)\n",
    "songs_trunc['flag'] = ((songs_trunc.SongID_2 != songs_trunc.SongID_2.shift()) |\n",
    "                                             (songs_trunc.SongID_2 != songs_trunc.SongID_2.shift(-1))).astype(int)\n",
    "#only save flag = 1 to a csv\n",
    "songs_cleanish = songs_trunc.loc[songs_trunc.flag == 1, :]\n",
    "songs_cleanish.to_csv('songs_cleanish_2.csv')\n",
    "\n",
    "\n",
    "'''Testing: \n",
    "songs_clean = songs.sort_values('SongID', 'Instance', 'Weeks On Chart')\n",
    "songs_sort_sub = songs_sort.iloc[320000:320494,:]\n",
    "#songs_sort_sub.to_csv('last subset.csv')\n",
    "week1228 = songs_sort.loc[songs_sort.WeekID == datetime(2019,12,28), :]\n",
    "#print(week1228)\n",
    "week1221 = songs_sort.loc[songs_sort.WeekID == datetime(2019,12,21),:]\n",
    "#print(week1221)\n",
    "#print(songs_sort.iloc[320400:320494,:])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning part 2: create debut_week in accordane for each row to be used for survival analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint 1\n",
    "songs = pd.read_csv('songs_cleanish_2.csv')\n",
    "\n",
    "print(songs.info())\n",
    "print(songs.describe())\n",
    "\n",
    "#flag the last row for every instance\n",
    "songs['flag_last'] = ((songs.SongID_2 != songs.SongID_2.shift(-1))).astype(int)\n",
    "\n",
    "\n",
    "#Create debut_week and debut_position conditions\n",
    "# ['WeekID'] of flag = 0 --> ['Debut Week'] for index + 1\n",
    "# ['Week Postion'] of flag = 0 --> ['Debut Position'] for index + 1\n",
    "\n",
    "cond1 = songs['flag_last'] == songs['flag']\n",
    "cond2 = songs['flag_last'] != songs['flag_last'].shift()\n",
    "cond3 = songs['flag_last'] == songs['flag_last'].shift()\n",
    "\n",
    "\n",
    "#Make debut_week and debut_position\n",
    "songs['Debut_Week_2'] = np.where((cond1) & (cond2),\n",
    "            songs['WeekID'].shift(),\n",
    "            np.NaN)\n",
    "songs['Debut_Week_3'] = np.where(cond3,\n",
    "            songs['WeekID'],\n",
    "            np.NaN)\n",
    "songs['Debut_Position_2'] = np.where((cond1) & (cond2),\n",
    "            songs['Week Position'].shift(),\n",
    "            np.NaN)\n",
    "songs['Debut_Position_3'] = np.where(cond3,\n",
    "            songs['Week Position'],\n",
    "            np.NaN)\n",
    "\n",
    "'''Testing: \n",
    "songs['flag_temp_check'] = ((songs.Debut_Week == songs.Debut_Week.shift())).astype(int)\n",
    "print(songs.flag_temp_check.sum())'''\n",
    "\n",
    "print(songs)\n",
    "\n",
    "songs.to_csv('songs_cleanish_4.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning part 3: consolidate debut_week, take care of right censoring, create month and year for later manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint 2\n",
    "songs = pd.read_csv('songs_cleanish_4.csv')\n",
    "\n",
    "#subset to only last distinct entry\n",
    "songs_sub = songs.loc[songs['flag_last'] == 1, :]\n",
    "print(songs_sub.info())\n",
    "\n",
    "#combining columns for debut_week and debut_position\n",
    "songs_sub['Debut_Week_2'].fillna(songs_sub['Debut_Week_3'], inplace = True)\n",
    "songs_sub['Debut_Position_2'].fillna(songs_sub['Debut_Position_3'], inplace = True)\n",
    "\n",
    "songs_sub.drop(columns = ['Previous Week Position', 'Debut_Week_3', 'Debut_Week', 'Debut_Position', 'Debut_Position_3'], inplace = True)\n",
    "\n",
    "songs_sub.rename(columns={'Debut_Week_2':'Debut_Week',\n",
    "                          'Debut_Position_2':'Debut_Position'},\n",
    "                 inplace=True)\n",
    "\n",
    "#plot weeks on chart to check Weibull distribution\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = songs_sub['Weeks on Chart'].plot.hist(bins = 50)\n",
    "plt.show()\n",
    "\n",
    "#right censoring, get rid of any weeks that have a last flag at week28 (12-28-2019)\n",
    "print(songs_sub.info())\n",
    "songs_sub = songs_sub.sort_values(by = 'WeekID')\n",
    "songs_sub = songs_sub.reset_index(drop = True)\n",
    "\n",
    "week28 = songs_sub.at[30797, 'WeekID']\n",
    "'''Testing: print(week28)'''\n",
    "songs_sub_2 = songs_sub.loc[songs_sub.WeekID != week28, :]\n",
    "print(songs_sub_2.info())\n",
    "\n",
    "#add Month and year for Debut week for controls later\n",
    "songs_sub_2.Debut_Week = pd.to_datetime(songs_sub_2.Debut_Week)\n",
    "songs_sub_2['Month'] = songs_sub_2['Debut_Week'].dt.strftime('%m')\n",
    "songs_sub_2['Year'] = songs_sub_2['Debut_Week'].dt.strftime('%Y')\n",
    "print(songs_sub_2.info())\n",
    "\n",
    "#famous variable - debut position should control for fame/marketing?\n",
    "#songs_sub_2.sort_values(by = ['Performer', 'Song'])\n",
    "\n",
    "songs_sub_2.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'], inplace = True)\n",
    "#print(songs_sub_2.info())\n",
    "songs_sub_2.to_csv('songs_cleanish_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning part 4: merging datasets, cleaning up extraneous variables for import into Stata and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('songs_cleanish_5.csv')\n",
    "keywords_sent = pd.read_csv('lyric-keywords-sent.csv')\n",
    "aud_features = pd.read_excel('Hot 100 Audio Features.xlsx')\n",
    "lyrics = pd.read_csv('lyric-full_df.csv')\n",
    "\n",
    "#import unemployment data and change to ints\n",
    "unemployment = pd.read_csv('UNRATE.csv')\n",
    "unemployment.DATE = pd.to_datetime(unemployment.DATE)\n",
    "unemployment['Month'] = unemployment['DATE'].dt.strftime('%m').astype(int)\n",
    "unemployment['Year'] = unemployment['DATE'].dt.strftime('%Y').astype(int)\n",
    "\n",
    "print(songs.info())\n",
    "print(lyrics.info())\n",
    "print(keywords_sent)\n",
    "print(aud_features.info())\n",
    "\n",
    "songs.WeekID = pd.to_datetime(songs.WeekID)\n",
    "\n",
    "#chose to implement later/in stata\n",
    "''''Indicator variables after 2000: recession and unemployment\n",
    "cond1 = songs['WeekID'] >= datetime(2007, 12, 1)\n",
    "cond2 = songs['WeekID'] <= datetime(2009, 5, 1)\n",
    "cond3 = songs['WeekID'] >= datetime(2009, 4, 1)\n",
    "cond4 = songs['WeekID'] <= datetime(2011, 8, 1)\n",
    "songs['Recession'] = np.where(cond1 & cond2, 1, 0)\n",
    "songs['Unemploy'] = np.where(cond3 & cond4, 1, 0)\n",
    "'''\n",
    "\n",
    "#merge with unemployment data\n",
    "songs_unemploy = pd.merge(songs, unemployment[['Month','Year', 'UNRATE']],\n",
    "                          on = ['Month', 'Year'],\n",
    "                          how = 'left',\n",
    "                          left_index=True)\n",
    "\n",
    "#merge with sentiment analysis, kept lyrics just in case for further NPL\n",
    "songs_lyrics = pd.merge(songs_unemploy,\n",
    "                        lyrics[['SongID','Lyrics', 'Sentiment']],\n",
    "                        on = 'SongID',\n",
    "                        how = 'left',\n",
    "                        left_index = True)\n",
    "\n",
    "#merge with Spotify audio features\n",
    "songs_all = pd.merge(songs_lyrics,\n",
    "                     aud_features[['SongID','spotify_genre','spotify_track_duration_ms','mode', 'valence', 'tempo',\n",
    "                                   'danceability','energy','speechiness', 'acousticness','loudness', 'key']],\n",
    "                     on = 'SongID',\n",
    "                     how = 'left',\n",
    "                     left_index = True)\n",
    "\n",
    "#clean up\n",
    "songs_all.drop(columns='Unnamed: 0', inplace=True)\n",
    "print(songs_all.info())\n",
    "songs_all = songs_all.drop_duplicates(keep='first')\n",
    "print(songs_all.info())\n",
    "\n",
    "#plot and check distribution\n",
    "songs_all.sort_values(by = 'Weeks on Chart')\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "songs_all['survival'] = np.log(songs_all['Weeks on Chart'])\n",
    "ax = songs_all['survival'].plot.hist(bins = 50)\n",
    "ax = songs_all['Weeks on Chart'].plot.hist(bins = 50)\n",
    "plt.show()\n",
    "\n",
    "#clean up for import into stata and SQL (no strings with commas)\n",
    "songs_all.drop(columns=['Lyrics', 'spotify_genre'], inplace=True)\n",
    "songs_all = songs_all.reset_index(drop = True)\n",
    "\n",
    "print(songs_all.size)\n",
    "songs_all.to_csv('songs_all_2.csv')\n",
    "\n",
    "#notes:\n",
    "#lyrics goes up to 2019-06-22\n",
    "#songs_all_2 = songs_all.reset_index().drop_duplicates(subset='index',\n",
    "                                 #      keep='first').set_index('index')\n",
    "'''Testing: \n",
    "nullsongs = songs_all_2[songs_all_2.isna().any()]\n",
    "print(nullsongs.info())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('songs_all_2.csv')\n",
    "print(songs.shape)\n",
    "songs = songs.sort_values(by = ['SongID', 'Instance'])\n",
    "songs = songs.reset_index(drop = True)\n",
    "\n",
    "#Weeks substracted for instances\n",
    "cond1 = songs['Instance'] > songs['Instance'].shift()\n",
    "songs['WeekChart2'] = np.where((cond1),\n",
    "            songs['Weeks on Chart']-songs['Weeks on Chart'].shift(),\n",
    "            songs['Weeks on Chart'])\n",
    "'''Testing: songs.survival = np.log(songs.WeekChart2)\n",
    "songs['WeekChart2'].plot.hist(bins = 100)\n",
    "songs['survival'].plot.hist(bins = 50)\n",
    "plt.show()'''\n",
    "\n",
    "#setting recession indicator\n",
    "songs.Debut_Week = pd.to_datetime((songs.Debut_Week))\n",
    "#print(songs.info())\n",
    "cond1 = (songs.Debut_Week > ('1960-04-01')) & (songs.Debut_Week < ('1961-02-01'))\n",
    "cond2 = (songs.Debut_Week > ('1970-01-01')) & (songs.Debut_Week < ('1970-11-01'))\n",
    "cond3 = (songs.Debut_Week > datetime(1973,12,1)) & (songs.Debut_Week < datetime(1975, 2,1))\n",
    "cond4 = (songs.Debut_Week > datetime(1980,1,1)) & (songs.Debut_Week < datetime(1980, 5,1))\n",
    "cond5 = (songs.Debut_Week > datetime(1981,8,1)) & (songs.Debut_Week < datetime(1982, 11,1))\n",
    "cond6 = (songs.Debut_Week > datetime(1990,8,1)) & (songs.Debut_Week < datetime(1991, 3,1))\n",
    "mask = (cond1 | cond2 | cond3 | cond4 | cond5 | cond6)\n",
    "songs['recession'] = np.where(mask, 1, 0)\n",
    "\n",
    "#export\n",
    "songs.to_csv('songs_all_3_2.csv')\n",
    "\n",
    "#restrict sample to before 1992 only due to week20 abnormality after 1992\n",
    "songs_temp = songs.loc[songs.WeekID < datetime(1992, 1, 1), :]\n",
    "print(songs_temp)\n",
    "songs_temp.to_csv('songs_before_1992.csv')\n",
    "\n",
    "'''Testing: print(songs.loc[songs.Song =='22', :])\n",
    "songs_temp.WeekChart2.plot.hist(bins = 80)\n",
    "plt.show()\n",
    "songs.WeekChart2.plot.hist(bins = 100)\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#taking out instances\n",
    "songs = pd.read_csv('songs_all_2.csv')\n",
    "songs = songs.sort_values(by = ['SongID', 'Instance'])\n",
    "songs = songs.reset_index(drop = True)\n",
    "\n",
    "cond2 = songs['Peak Position'] > songs['Peak Position'].shift()\n",
    "cond3 = songs['Song'] == songs['Song'].shift()\n",
    "\n",
    "songs['Peak'] = np.where((cond2) & (cond3),\n",
    "                         songs['Peak Position'].shift(),\n",
    "                         songs['Peak Position'])\n",
    "\n",
    "print(songs.shape)\n",
    "songs = songs.sort_values(by = ['SongID', 'Instance'])\n",
    "songs.flag_last = ((songs.SongID != songs.SongID.shift(-1))).astype(int)\n",
    "songs = songs.loc[songs.flag_last == 1, :]\n",
    "\n",
    "#export various\n",
    "songs.to_csv('songs_all_without_instances.csv')\n",
    "\n",
    "songs.WeekID = pd.to_datetime(songs.WeekID)\n",
    "songs_before = songs.loc[songs.WeekID < datetime(1992, 1, 1), :]\n",
    "songs_before.to_csv('songs_before_1992 without_instances.csv')\n",
    "\n",
    "songs_before = songs_before.loc[songs['Peak'] <= 10, :]\n",
    "songs_before.to_csv('top10_songs_before_1992_without_instances.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
